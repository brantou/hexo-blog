#+TITLE: 玩转词云
#+DATE: <2017-08-22 Tue>
#+LAYOUT: post
#+OPTIONS: ':t author:nil ^:{}
#+TAGS: python, jieba, wordcloud
#+CATEGORIES: 技术积累
#+STARTUP: content

* jieba中文分词--做最好的Python中文分词组件
  :PROPERTIES:
  :ID:       681fffd0-61dc-44eb-860a-82cfbfe2c080
  :END:
** 分词
   :PROPERTIES:
   :ID:       a7a6f9a3-eee0-4e7a-8ec2-a3281cbdb2ba
   :END:
  #+NAME: jieba-cut
  #+BEGIN_SRC python :preamble "# -*- coding: utf-8 -*-" :results output :var srcWd="做最好的Python中文分词组件"
    import jieba

    seg_list = jieba.cut(srcWd, cut_all=True)
    print(u'Full mode:' + u'/ '.join(seg_list))

    seg_list = jieba.cut(srcWd, cut_all=False)
    print(u'Default mode:' + u'/ '.join(seg_list))

    seg_list = jieba.cut_for_search(srcWd)
    print(u', '.join(seg_list))
  #+END_SRC

  #+RESULTS:
  : Full mode:做/ 最好/ 的/ Python/ 中文/ 分词/ 词组/ 组件
  : Default mode:做/ 最好/ 的/ Python/ 中文/ 分词/ 组件
  : 做, 最好, 的, Python, 中文, 分词, 组件

  #+call: jieba-cut(srcWd="我目前在学习Golang和Python。")

  #+RESULTS:
  : Full mode:我/ 目前/ 在/ 学习/ Golang/ 和/ Python/
  : Default mode:我/ 目前/ 在/ 学习/ Golang/ 和/ Python/ 。
  : 我, 目前, 在, 学习, Golang, 和, Python, 。
** 自定义词典
   :PROPERTIES:
   :ID:       ba473993-dd11-4116-8578-b31a6161ed0e
   :END:
   招聘信息收集了很多，但是关键字提取却不是很理想，我查看了jiaba的词典里，基本没有对英文专有名词的支持，
   这可苦煞了我，但是jieba支持自定义词典，我来生成英文的专有名词的词典好了。一行一行的加，难为我这个懒人了，
   但总有好心人，已经整理好了，试想这个名词是不是招聘网站已经整理好了，而且分门别类的，只需要把它们取回来就好了。
   #+NAME: get-soup
   #+BEGIN_SRC python
     def get_html(url):
         request = urllib2.Request(url)
         request.add_header(
             'User-Agent',
             'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36'
         )
         html = urllib2.urlopen(request)
         return html.read()

     def get_soup(url):
         soup = BeautifulSoup(get_html(url), 'lxml')
         return soup
   #+END_SRC

   #+BEGIN_SRC python :preamble "# -*- coding: utf-8 -*-" :results output :noweb strip-export
     def fetch_lagou():
         words = []
         url = 'https://www.lagou.com/'
         soup = get_soup(url)
         category_list = soup.find_all('div', attrs={'class': 'menu_sub dn'})
         for category in category_list:
             dls = category.find_all('dl')
             for dl in dls:
                 names = dl.dd.find_all('a')
                 for name in names:
                     words.append(name.text)
         return words


     def fetch_zhipin():
         words = []
         url = 'http://www.zhipin.com/'
         soup = get_soup(url)
         job_menu = soup.find('div', attrs={'class': 'job-menu'})
         dls = job_menu.find_all('dl')
         for dl in dls:
             divs = dl.find_all('div', attrs={'class': 'text'})
             for div in divs:
                 names = div.find_all('a')
                 for name in names:
                     words.append(name.text)
         return words
   #+END_SRC

   看来我思维受限了，招聘网站整理的信息都是高频信息，但是广度肯定是不够的，那怎么办呢。
   这个时候 [[https://stackoverflow.com/tags?page=1&tab=popular][stackoverflow]] 闪现在我的脑海中，没有那个网站里有关技术的广度，能和它匹敌了。
   #+NAME: fetch-stackoverflow
   #+BEGIN_SRC python
       def fetch_stackoverflow():
         words = []
         for pageNo in range(1, 20):
             url = 'https://stackoverflow.com/tags?page=%d&tab=popular' % (pageNo)
             soup = get_soup(url)
             tags_list = soup.find('div', attrs={'id': 'tags_list'})
             trs = tags_list.table.find_all('tr')
             for tr in trs:
                 tds = tr.find_all('td')
                 for td in tds:
                     words.append(td.a.text)
         return words
   #+END_SRC

** 调整词典
   :PROPERTIES:
   :ID:       0f9f838d-0b96-4d20-bbd0-f9b31664e187
   :END:

   #+NAME: jieba-freq
   #+BEGIN_SRC python :preamble "# -*- coding: utf-8 -*-" :results output :var srcWd="做最好的Python中文分词组件"
     import jieba
     import jieba.analyse

     print(', '.join(
         jieba.analyse.extract_tags(srcWd, allowPOS=('n', 'nz', 'eng', 'vn'))))
     jieba.load_userdict('../resource/userdict.txt')
     print(', '.join(
         jieba.analyse.extract_tags(srcWd, allowPOS=('n', 'nz', 'eng', 'vn'))))
   #+END_SRC

   #+RESULTS: jieba-freq
   : Python, 分词, 组件, 中文
   : Python, 分词, 组件, 中文

   #+call: jieba-freq(srcWd="1. 扎实的计算机学科基础 2. 数据仓库和数据建模工作经验三年以上，良好的业务理解能力和模型抽象能力 3. 有海量大数据平台使用经验，熟悉hadoop、odps、hive等平台的使用 4. 有数据挖掘经验的优先考虑 5. 具有良好的沟通和团队协作能力，对业务有良好的理解能力和敏锐度")

   #+RESULTS:
   : 理解能力, 经验, 数据挖掘, 敏锐度, 平台, odps, hadoop, 数据仓库, hive, 建模, 业务, 海量, 能力, 数据, 协作, 优先, 团队, 模型, 计算机, 学科
   : 理解能力, 经验, 数据挖掘, 敏锐度, 平台, odps, hadoop, 数据仓库, hive, 建模, 业务, 海量, 能力, 数据, 协作, 优先, 团队, 模型, 计算机, 学科

** 关键词提取
   :PROPERTIES:
   :ID:       7308abe6-a074-4153-a4c0-bb74f199bb07
   :END:
   #+NAME: jieba-analyse
   #+BEGIN_SRC python :preamble "# -*- coding: utf-8 -*-" :results output :var srcWd="做最好的Python中文分词组件"
     import jieba.analyse

     print(', '.join(jieba.analyse.extract_tags(srcWd, allowPOS=())))
     print(', '.join(
         jieba.analyse.extract_tags(srcWd, allowPOS=('n', 'nt', 'nz', 'eng', 'vn'
                                                     ))))
     wordlst = jieba.analyse.extract_tags(
         srcWd, withWeight=True, allowPOS=('n', 'nt', 'nz', 'eng', 'vn'))

     for word in wordlst:
         print(word[0] + ": " + str(word[1]))
   #+END_SRC

   #+RESULTS: jieba-analyse
   : Python, 分词, 组件, 中文, 最好
   : Python, 分词, 组件, 中文
   : Python: 2.98869187572
   : 分词: 2.92586326865
   : 组件: 2.34472226873
   : 中文: 2.03702625329

   #+call: jieba-analyse(srcWd="1. 扎实的计算机学科基础 2. 数据仓库和数据建模工作经验三年以上，良好的业务理解能力和模型抽象能力 3. 有海量大数据平台使用经验，熟悉hadoop、odps、hive等平台的使用 4. 有数据挖掘经验的优先考虑 5. 具有良好的沟通和团队协作能力，对业务有良好的理解能力和敏锐度")

   #+NAME: jieba-analyse-tr
   #+BEGIN_SRC python :preamble "# -*- coding: utf-8 -*-" :results output :var srcWd="做最好的Python中文分词组件"
     import jieba.analyse

     print(', '.join(jieba.analyse.textrank(srcWd)))
     print(', '.join(
         jieba.analyse.textrank(srcWd, allowPOS=('n', 'nz', 'eng', 'vn'))))
   #+END_SRC

   #+RESULTS: jieba-analyse-tr
   : 组件, 分词
   : 组件, 分词, 中文, Python

   #+call: jieba-analyse-tr(srcWd="1. 扎实的计算机学科基础 2. 数据仓库和数据建模工作经验三年以上，良好的业务理解能力和模型抽象能力 3. 有海量大数据平台使用经验，熟悉hadoop、odps、hive等平台的使用 4. 有数据挖掘经验的优先考虑 5. 具有良好的沟通和团队协作能力，对业务有良好的理解能力和敏锐度")

** 词性标注
   :PROPERTIES:
   :ID:       ba65f4b7-159b-440a-9df3-ac90465716e4
   :END:
   #+NAME: jieba-posseg
   #+BEGIN_SRC python :preamble "# -*- coding: utf-8 -*-" :results output :var srcWd="做最好的Python中文分词组件"
     import jieba
     import jieba.posseg as pseg

     jieba.load_userdict('../resource/userdict.txt')
     words = pseg.cut(srcWd)
     for word, flag in words:
         print('%s %s' % (word, flag))
   #+END_SRC

   #+RESULTS: jieba-posseg
   : 做 v
   : 最好 a
   : 的 uj
   : Python nz
   : 中文 nz
   : 分词 n
   : 组件 n

   #+call: jieba-posseg(srcWd="1. 扎实的计算机学科基础 2. 数据仓库和数据建模工作经验三年以上，良好的业务理解能力和模型抽象能力 3. 有海量大数据平台使用经验，熟悉hadoop、odps、hive等平台的使用 4. 有数据挖掘经验的优先考虑 5. 具有良好的沟通和团队协作能力，对业务有良好的理解能力和敏锐度")

* 词云
  :PROPERTIES:
  :ID:       d3a8f6d2-c043-4104-9f83-08e71a7a5c7a
  :END:

** 简单词云--美国宪法的词云
   :PROPERTIES:
   :ID:       9133f12f-f7ba-4019-b21f-fbdea70088b1
   :END:
   #+BEGIN_SRC python :preamble "# -*- coding: utf-8 -*-" :results output slient
     """
     Minimal Example
     ===============
     Generating a square wordcloud from the US constitution using default arguments.
     """

     from wordcloud import WordCloud

     # Read the whole text.
     text = open('../resource/constitution.txt').read()

     # Generate a word cloud image
     wordcloud = WordCloud(width=640, height=480).generate(text)
     wordcloud.to_file('../images/constitution-n.png')

     wordcloud = WordCloud(width=640, height=480, max_font_size=80).generate(text)
     wordcloud.to_file('../images/constitution-s.png')
   #+END_SRC

   #+BEGIN_EXPORT html
   <img src="/images/constitution-n.png" />
   #+END_EXPORT

   #+BEGIN_EXPORT html
   <img src="/images/constitution-s.png" />
   #+END_EXPORT

** Colored by Group
   :PROPERTIES:
   :ID:       74f1f64c-c027-4bd1-bdee-48e68b59bc85
   :END:
   #+BEGIN_SRC python :preamble "# -*- coding: utf-8 -*-" :results output slient
     from wordcloud import (WordCloud, get_single_color_func)
     import matplotlib.pyplot as plt


     class SimpleGroupedColorFunc(object):
         """Create a color function object which assigns EXACT colors
            to certain words based on the color to words mapping

            Parameters
            ----------
            color_to_words : dict(str -> list(str))
              A dictionary that maps a color to the list of words.

            default_color : str
              Color that will be assigned to a word that's not a member
              of any value from color_to_words.
         """

         def __init__(self, color_to_words, default_color):
             self.word_to_color = {
                 word: color
                 for (color, words) in color_to_words.items() for word in words
             }

             self.default_color = default_color

         def __call__(self, word, **kwargs):
             return self.word_to_color.get(word, self.default_color)


     class GroupedColorFunc(object):
         """Create a color function object which assigns DIFFERENT SHADES of
            specified colors to certain words based on the color to words mapping.

            Uses wordcloud.get_single_color_func

            Parameters
            ----------
            color_to_words : dict(str -> list(str))
              A dictionary that maps a color to the list of words.

            default_color : str
              Color that will be assigned to a word that's not a member
              of any value from color_to_words.
         """

         def __init__(self, color_to_words, default_color):
             self.color_func_to_words = [(get_single_color_func(color), set(words))
                                         for (color,
                                              words) in color_to_words.items()]

             self.default_color_func = get_single_color_func(default_color)

         def get_color_func(self, word):
             """Returns a single_color_func associated with the word"""
             try:
                 color_func = next(
                     color_func for (color_func, words) in self.color_func_to_words
                     if word in words)
             except StopIteration:
                 color_func = self.default_color_func

             return color_func

         def __call__(self, word, **kwargs):
             return self.get_color_func(word)(word, **kwargs)


     text = """The Zen of Python, by Tim Peters
     Beautiful is better than ugly.
     Explicit is better than implicit.
     Simple is better than complex.
     Complex is better than complicated.
     Flat is better than nested.
     Sparse is better than dense.
     Readability counts.
     Special cases aren't special enough to break the rules.
     Although practicality beats purity.
     Errors should never pass silently.
     Unless explicitly silenced.
     In the face of ambiguity, refuse the temptation to guess.
     There should be one-- and preferably only one --obvious way to do it.
     Although that way may not be obvious at first unless you're Dutch.
     Now is better than never.
     Although never is often better than *right* now.
     If the implementation is hard to explain, it's a bad idea.
     If the implementation is easy to explain, it may be a good idea.
     Namespaces are one honking great idea -- let's do more of those!"""

     # Since the text is small collocations are turned off and text is lower-cased
     wc = WordCloud(collocations=False).generate(text.lower())

     color_to_words = {
         # words below will be colored with a green single color function
         '#00ff00': [
             'beautiful', 'explicit', 'simple', 'sparse', 'readability', 'rules',
             'practicality', 'explicitly', 'one', 'now', 'easy', 'obvious', 'better'
         ],
         # will be colored with a red single color function
         'red': [
             'ugly', 'implicit', 'complex', 'complicated', 'nested', 'dense',
             'special', 'errors', 'silently', 'ambiguity', 'guess', 'hard'
         ]
     }

     # Words that are not in any of the color_to_words values
     # will be colored with a grey single color function
     default_color = 'grey'

     # Create a color function with single tone
     # grouped_color_func = SimpleGroupedColorFunc(color_to_words, default_color)

     # Create a color function with multiple tones
     grouped_color_func = GroupedColorFunc(color_to_words, default_color)

     # Apply our color function
     wc.recolor(color_func=grouped_color_func)
     wc.to_file('../images/grouped-color.png')
   #+END_SRC

   #+RESULTS:

   #+BEGIN_EXPORT html
   <img src="/images/grouped-color.png" />
   #+END_EXPORT

** 西游记的词云
   :PROPERTIES:
   :ID:       a83e73b6-c132-4eb3-b968-d985cd2a5ceb
   :END:
   #+BEGIN_SRC python :preamble "# -*- coding: utf-8 -*-" :results output slient
     import jieba.analyse
     from wordcloud import WordCloud, ImageColorGenerator
     import numpy as np
     from PIL import Image
     import random


     def grey_color_func(word,
                         font_size,
                         position,
                         orientation,
                         random_state=None,
                         ,**kwargs):
         return "hsl(0, 0%%, %d%%)" % random.randint(60, 100)


     font_path = '../resource/tyzkaishu.ttf'
     width = 640
     height = 480

     text = open('../resource/xiyouji.txt').read()
     words = jieba.analyse.extract_tags(text, topK=200, withWeight=True)

     word_freqs = {}
     for word in words:
         word_freqs[word[0]] = word[1]

     wordcloud = WordCloud(
         font_path=font_path, width=width,
         height=height).generate_from_frequencies(word_freqs)
     wordcloud.to_file('../images/xiyouji.png')

     mask = np.array(Image.open('../resource/stormtrooper_mask.png'))
     wordcloud = WordCloud(
         font_path=font_path, width=width, height=height,
         mask=mask).generate_from_frequencies(word_freqs)
     wordcloud.to_file('../images/xiyouji-mask.png')

     # recolor wordcloud
     wordcloud.recolor(color_func=grey_color_func, random_state=3)
     wordcloud.to_file('../images/xiyouji-custom.png')

     alice_coloring = np.array(Image.open('../resource/alice_color.png'))
     wordcloud = WordCloud(
         font_path=font_path,
         width=width,
         height=height,
         background_color="white",
         mask=alice_coloring,
         max_font_size=80,
         random_state=42).generate_from_frequencies(word_freqs)

     # create coloring from image
     image_colors = ImageColorGenerator(alice_coloring)

     # recolor wordcloud
     wordcloud.recolor(color_func=image_colors)
     wordcloud.to_file('../images/xiyouji-color.png')
   #+END_SRC

   #+BEGIN_EXPORT html
   <img src="/images/xiyouji.png" />
   #+END_EXPORT

   #+BEGIN_EXPORT html
   <img src="/images/xiyouji-mask.png" />
   #+END_EXPORT

   #+BEGIN_EXPORT html
   <img src="/images/xiyouji-custom.png" />
   #+END_EXPORT

   #+BEGIN_EXPORT html
   <img src="/images/xiyouji-color.png" />
   #+END_EXPORT

** 阿里招聘的词云
   招聘信息是我使用爬虫趴下来的的，这里只做数据的分析。
   #+BEGIN_SRC python :preamble "# -*- coding: utf-8 -*-" :results output slient
     import jieba.analyse
     import jieba
     from wordcloud import WordCloud
     import numpy as np
     from PIL import Image
     from pymongo import MongoClient

     font_path = '../resource/tyzkaishu.ttf'
     width = 640
     height = 480

     client = MongoClient('mongodb://localhost:27017/')
     jobs = client.jobs.alibaba
     text = ''
     for job in jobs.find():
         if job.get('description'):
             text += job.get('description').replace('<br/>', ' ') + '\n'

     jieba.load_userdict('../resource/userdict.txt')
     words = jieba.analyse.extract_tags(text, topK=200, withWeight=True)
     word_freqs = {}
     for word in words:
         word_freqs[word[0]] = word[1]

     mask = np.array(Image.open('../resource/stormtrooper_mask.png'))
     wordcloud = WordCloud(
         font_path=font_path, width=width, height=height,
         mask=mask).generate_from_frequencies(word_freqs)
     wordcloud.to_file('../images/alibaba-mask.png')
   #+END_SRC
